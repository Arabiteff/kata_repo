{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-14 14:32:27,168 - INFO - Step 1: Ingest Flights Finished\n",
      "2024-12-14 14:32:27,427 - INFO - Step 2: Ingest Airports Finished\n",
      "2024-12-14 14:32:27,644 - INFO - Step 3: Ingest Airlines Finished\n",
      "2024-12-14 14:32:37,895 - INFO - Step 4: Ingest Zones Finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-----+-------+------+------+\n",
      "|    continent|    subzone| tl_y|   tl_x|  br_y|  br_x|\n",
      "+-------------+-----------+-----+-------+------+------+\n",
      "|       europe|     poland|56.86|  11.06| 48.22| 28.26|\n",
      "|       europe|    germany|57.92|   1.81| 45.81| 16.83|\n",
      "|       europe|         uk|62.61| -13.07| 49.71|  3.46|\n",
      "|       europe|      spain|44.36| -11.06| 35.76|  4.04|\n",
      "|       europe|     france|51.07|  -5.18| 42.17|   8.9|\n",
      "|       europe|       ceur|51.39|  11.25| 39.72| 32.55|\n",
      "|       europe|scandinavia|72.12|  -0.73| 53.82| 40.67|\n",
      "|       europe|      italy|47.67|   5.26| 36.27| 20.64|\n",
      "| northamerica|       na_n|72.82|-177.97| 41.92|-52.48|\n",
      "| northamerica|       na_c|54.66|-134.68| 22.16|-56.91|\n",
      "| northamerica|       na_s|41.92|-177.83|  3.82|-52.48|\n",
      "| southamerica|       NULL| 16.0|  -96.0| -57.0| -31.0|\n",
      "|      oceania|       NULL|19.62|   88.4|-55.08| 180.0|\n",
      "|         asia|      japan|60.38|  113.5| 22.58|176.47|\n",
      "|       africa|       NULL| 39.0|  -29.0| -39.0|  55.0|\n",
      "|     atlantic|       NULL|52.62|  -50.9| 15.62| -4.75|\n",
      "|     maldives|       NULL|10.72|   63.1| -6.08| 86.53|\n",
      "|northatlantic|       NULL|82.62| -84.53| 59.02|  4.45|\n",
      "+-------------+-----------+-----+-------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from FlightRadar24 import FlightRadar24API\n",
    "from pyspark.sql import SparkSession\n",
    "from utils.calculate_distance import calculate_distance\n",
    "from utils.map_zone import map_zone\n",
    "import os\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import json\n",
    "from config import airport_schema, flight_schema, airline_schema, zone_schema  # Import schemas\n",
    "\n",
    "os.environ['JAVA_HOME'] = 'C:/Program Files/Java/jdk-11'  # Update to your Java path\n",
    "os.environ['SPARK_HOME'] = 'C:/spark'  # Update to your Spark installation path\n",
    "os.environ['HADOOP_HOME'] = 'C:/hadoop'  # Required for Windows\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"C:\\\\Users\\\\tefte\\\\anaconda3\\\\envs\\\\kata-env\\\\python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"C:\\\\Users\\\\tefte\\\\anaconda3\\\\envs\\\\kata-env\\\\python\"\n",
    "\n",
    "class FlightPipeline:\n",
    "    def __init__(self):\n",
    "        self.fr_api = FlightRadar24API()\n",
    "        self.spark = SparkSession.builder.appName(\"FlightPipeline\").getOrCreate()\n",
    "        self.df_flights = None\n",
    "        self.df_airports = None\n",
    "        self.df_airlines = None\n",
    "        self.df_zones = None\n",
    "\n",
    "        # Initialize logging\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "            handlers=[\n",
    "                logging.FileHandler(\"pipeline.log\"),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def extract(self):\n",
    "        try:\n",
    "            # Step 1: Ingest Flights\n",
    "            flights = self.fr_api.get_flights()\n",
    "            data_flights = []\n",
    "            for flight in flights:\n",
    "                try:\n",
    "                    data_flights.append({\n",
    "                        \"flightID\": flight.id,\n",
    "                        \"aircraft\": flight.aircraft_code,\n",
    "                        \"airlineCode\": flight.airline_icao,\n",
    "                        \"airportOrigineCode\": flight.origin_airport_iata,\n",
    "                        \"airportDestinationCode\": flight.destination_airport_iata,\n",
    "                        \"flightStatus\": flight.on_ground\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error retrieving flight details for flight {flight}: {e}\")\n",
    "            self.df_flights = self.spark.createDataFrame(data_flights, schema=flight_schema)  # Use imported schema\n",
    "            # self.df_flights.show()\n",
    "            logging.info('Step 1: Ingest Flights Finished')\n",
    "\n",
    "            # Step 2: Ingest Airports\n",
    "            airports = self.fr_api.get_airports()\n",
    "            data_airports = []\n",
    "            for airport in airports:\n",
    "                try:\n",
    "                    data_airports.append({\n",
    "                        \"airportID\": airport.iata,\n",
    "                        \"airportName\": airport.name,\n",
    "                        \"airportLat\": float(airport.latitude) if airport.latitude else None,\n",
    "                        \"airportLong\": float(airport.longitude) if airport.longitude else None,\n",
    "                        \"airportCountryName\": airport.country,\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error retrieving airport details for airport {airport}: {e}\")\n",
    "            self.df_airports = self.spark.createDataFrame(data_airports, schema=airport_schema)  # Use imported schema\n",
    "            # self.df_airports.show()\n",
    "            logging.info('Step 2: Ingest Airports Finished')\n",
    "\n",
    "            # Step 3: Ingest Airlines\n",
    "            data_airlines = self.fr_api.get_airlines()\n",
    "            self.df_airlines = self.spark.createDataFrame(data_airlines, schema=airline_schema)  # Use imported schema\n",
    "            # self.df_airlines.show()\n",
    "            logging.info('Step 3: Ingest Airlines Finished')\n",
    "\n",
    "            # Step 4: Ingest Zones\n",
    "            zones = self.fr_api.get_zones()\n",
    "            data_zones = []\n",
    "            for continent, details in zones.items():\n",
    "                if \"subzones\" in details:\n",
    "                    for subzone, sub_details in details[\"subzones\"].items():\n",
    "                        data_zones.append({\n",
    "                            \"continent\": continent,\n",
    "                            \"subzone\": subzone,\n",
    "                            \"tl_y\": float(sub_details[\"tl_y\"]),\n",
    "                            \"tl_x\": float(sub_details[\"tl_x\"]),\n",
    "                            \"br_y\": float(sub_details[\"br_y\"]),\n",
    "                            \"br_x\": float(sub_details[\"br_x\"])\n",
    "                        })\n",
    "                else:\n",
    "                    data_zones.append({\n",
    "                        \"continent\": continent,\n",
    "                        \"subzone\": None,\n",
    "                        \"tl_y\": float(details[\"tl_y\"]),\n",
    "                        \"tl_x\": float(details[\"tl_x\"]),\n",
    "                        \"br_y\": float(details[\"br_y\"]),\n",
    "                        \"br_x\": float(details[\"br_x\"])\n",
    "                    })\n",
    "            self.df_zones = self.spark.createDataFrame(data_zones, schema=zone_schema)  # Use imported schema\n",
    "            self.df_zones.show()\n",
    "            logging.info('Step 4: Ingest Zones Finished')\n",
    "\n",
    "            # Return all DataFrames\n",
    "            return self.df_flights, self.df_airports, self.df_airlines, self.df_zones\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Critical error during ingestion: {e}\")\n",
    "            return None, None, None, None\n",
    "\n",
    "\n",
    "# Example usage\n",
    "pipeline = FlightPipeline()\n",
    "df_flights, df_airports, df_airlines, df_zones = pipeline.extract()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame successfully saved to ./data/ingested_data\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"myapp\").getOrCreate()\n",
    "output_path = \"./data/ingested_data\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "df_zones.write.mode(\"overwrite\").parquet(output_path)\n",
    "print(f\"DataFrame successfully saved to {output_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.4\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"myapp\").getOrCreate()\n",
    "\n",
    "print(spark.sparkContext._jvm.org.apache.hadoop.util.VersionInfo.getVersion())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/results/airline_with_most_flights\\tech_year=2024\n",
      "+-----------+------------+------------+\n",
      "|airlineCode|airline_name|flight_count|\n",
      "+-----------+------------+------------+\n",
      "|        UAE|    Emirates|          89|\n",
      "+-----------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "\n",
    "os.environ['JAVA_HOME'] = 'C:/Program Files/Java/jdk-11'  # Update to your Java path\n",
    "os.environ['SPARK_HOME'] = 'C:/spark'  # Update to your Spark installation path\n",
    "os.environ['HADOOP_HOME'] = 'C:/hadoop'  # Required for Windows\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"C:\\\\Users\\\\tefte\\\\anaconda3\\\\envs\\\\kata-env\\\\python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"C:\\\\Users\\\\tefte\\\\anaconda3\\\\envs\\\\kata-env\\\\python\"\n",
    "# Get the current datetime\n",
    "now = datetime.now()\n",
    "tech_year = now.strftime(\"%Y\")\n",
    "tech_month = now.strftime(\"%m\")\n",
    "tech_day = now.strftime(\"%d\")\n",
    "tech_hour = now.strftime(\"%H\")\n",
    "\n",
    "\n",
    "# Step 1: Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ReadResultsData\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Base path to the results folder\n",
    "base_results_path = \"./data/results/airline_with_most_flights\"\n",
    "\n",
    "# Step 3: Dynamically detect the latest folder\n",
    "latest_path = max(\n",
    "    [os.path.join(base_results_path, d) for d in os.listdir(base_results_path)],\n",
    "    key=os.path.getmtime\n",
    ")\n",
    "\n",
    "# Step 4: Load the Parquet data\n",
    "print(latest_path)\n",
    "df_results = spark.read.parquet(f\"{base_results_path}/tech_year={tech_year}/tech_month={tech_month}/tech_day={tech_day}/tech_hour={tech_hour}\")\n",
    "\n",
    "# Step 5: Show the loaded data\n",
    "df_results.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/results/airline_with_most_regional_flights\\tech_year=2024\n",
      "+---------------+-----------+------------------+---------------------+\n",
      "|originContinent|airlineCode|      airline_name|regional_flight_count|\n",
      "+---------------+-----------+------------------+---------------------+\n",
      "|         africa|        ETH|Ethiopian Airlines|                    7|\n",
      "|           asia|        CHH|   Hainan Airlines|                    2|\n",
      "|         europe|        TAP|  TAP Air Portugal|                    2|\n",
      "|   northamerica|        AAL| American Airlines|                   20|\n",
      "|        oceania|        QFA|            Qantas|                    8|\n",
      "|   southamerica|        GLO| GOL Linhas Aereas|                    1|\n",
      "+---------------+-----------+------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "\n",
    "os.environ['JAVA_HOME'] = 'C:/Program Files/Java/jdk-11'  # Update to your Java path\n",
    "os.environ['SPARK_HOME'] = 'C:/spark'  # Update to your Spark installation path\n",
    "os.environ['HADOOP_HOME'] = 'C:/hadoop'  # Required for Windows\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"C:\\\\Users\\\\tefte\\\\anaconda3\\\\envs\\\\kata-env\\\\python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"C:\\\\Users\\\\tefte\\\\anaconda3\\\\envs\\\\kata-env\\\\python\"\n",
    "# Get the current datetime\n",
    "now = datetime.now()\n",
    "tech_year = now.strftime(\"%Y\")\n",
    "tech_month = now.strftime(\"%m\")\n",
    "tech_day = now.strftime(\"%d\")\n",
    "tech_hour = now.strftime(\"%H\")\n",
    "\n",
    "\n",
    "# Step 1: Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ReadResultsData\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Base path to the results folder\n",
    "base_results_path = \"./data/results/airline_with_most_regional_flights\"\n",
    "\n",
    "# Step 3: Dynamically detect the latest folder\n",
    "latest_path = max(\n",
    "    [os.path.join(base_results_path, d) for d in os.listdir(base_results_path)],\n",
    "    key=os.path.getmtime\n",
    ")\n",
    "\n",
    "# Step 4: Load the Parquet data\n",
    "print(latest_path)\n",
    "df_results = spark.read.parquet(f\"{base_results_path}/tech_year={tech_year}/tech_month={tech_month}/tech_day={tech_day}/tech_hour={tech_hour}\")\n",
    "\n",
    "# Step 5: Show the loaded data\n",
    "df_results.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/results/Aircraft_manufacturer_with_most_active_flights\\tech_year=2024\n",
      "+--------+-------------------+\n",
      "|aircraft|active_flight_count|\n",
      "+--------+-------------------+\n",
      "|    B77W|                202|\n",
      "+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "\n",
    "os.environ['JAVA_HOME'] = 'C:/Program Files/Java/jdk-11'  # Update to your Java path\n",
    "os.environ['SPARK_HOME'] = 'C:/spark'  # Update to your Spark installation path\n",
    "os.environ['HADOOP_HOME'] = 'C:/hadoop'  # Required for Windows\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"C:\\\\Users\\\\tefte\\\\anaconda3\\\\envs\\\\kata-env\\\\python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"C:\\\\Users\\\\tefte\\\\anaconda3\\\\envs\\\\kata-env\\\\python\"\n",
    "# Get the current datetime\n",
    "now = datetime.now()\n",
    "tech_year = now.strftime(\"%Y\")\n",
    "tech_month = now.strftime(\"%m\")\n",
    "tech_day = now.strftime(\"%d\")\n",
    "tech_hour = now.strftime(\"%H\")\n",
    "\n",
    "\n",
    "# Step 1: Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ReadResultsData\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Base path to the results folder\n",
    "base_results_path = \"./data/results/Aircraft_manufacturer_with_most_active_flights\"\n",
    "\n",
    "# Step 3: Dynamically detect the latest folder\n",
    "latest_path = max(\n",
    "    [os.path.join(base_results_path, d) for d in os.listdir(base_results_path)],\n",
    "    key=os.path.getmtime\n",
    ")\n",
    "\n",
    "# Step 4: Load the Parquet data\n",
    "print(latest_path)\n",
    "df_results = spark.read.parquet(f\"{base_results_path}/tech_year={tech_year}/tech_month={tech_month}/tech_day={tech_day}/tech_hour={tech_hour}\")\n",
    "\n",
    "# Step 5: Show the loaded data\n",
    "df_results.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 03:16:59,223 - INFO - Loading data for flights from ./data/ingested_data\\flights\\tech_year=2024\\tech_month=12\\tech_day=15\\tech_hour=03\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, row_number, count\n",
    "from pyspark.sql.window import Window\n",
    "from utils.load_latest_data import load_latest_data\n",
    "from utils.save_kpi_data import save_kpi_data\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"AircraftManufacturerWithMostActiveFlights\").getOrCreate()\n",
    "\n",
    "# Base paths for loading and saving data\n",
    "base_extracted_path =\"./data/ingested_data\"\n",
    "results_path = \"./data/results\"\n",
    "df_flights = load_latest_data(spark, base_extracted_path, \"flights\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----------+------------------+----------------------+------------+\n",
      "|flightID|aircraft|airlineCode|airportOrigineCode|airportDestinationCode|flightStatus|\n",
      "+--------+--------+-----------+------------------+----------------------+------------+\n",
      "|3857547f|    B77W|        QTR|               DOH|                   HKG|           0|\n",
      "|38581e7d|    A359|        ETH|               PVG|                   ADD|           0|\n",
      "|38583115|    A359|        DLH|               PVG|                   MUC|           0|\n",
      "|38583161|    B789|        MSR|               PVG|                   CAI|           0|\n",
      "|38583688|    A359|        SIA|               SIN|                   JFK|           0|\n",
      "|3858372c|    B77W|        THY|               ICN|                   IST|           0|\n",
      "|3858376c|    A359|        SJX|               TPE|                   SFO|           0|\n",
      "|38583d05|    A359|        CCA|               PEK|                   MEL|           0|\n",
      "|3858403d|    B789|        CSH|               PVG|                   BUD|           0|\n",
      "|3858424a|    B77W|        THY|               TPE|                   IST|           0|\n",
      "|385842a7|    AT75|        NVQ|               DAC|                   CXB|           0|\n",
      "|385860b2|    B748|        CCA|               PEK|                   JFK|           0|\n",
      "|38586865|    B789|        AFR|               PEK|                   CDG|           0|\n",
      "|38587304|    B789|        QFA|               PER|                   LHR|           0|\n",
      "|385873ba|    B789|        DKH|               PVG|                   BRU|           0|\n",
      "|38587541|    B77W|        AFR|               PVG|                   CDG|           0|\n",
      "|38587633|    B77W|        CES|               PVG|                   CDG|           0|\n",
      "|3858768b|    B788|        KQA|               NBO|                   JFK|           0|\n",
      "|385876c7|    A359|        CES|               PVG|                   SYD|           0|\n",
      "|38587d8d|    B789|        CSN|               CAN|                   LAX|           0|\n",
      "+--------+--------+-----------+------------------+----------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "active_flights = df_flights.filter(col(\"flightStatus\") == \"0\")\n",
    "active_flights.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_manufacturer = (\n",
    "    active_flights.groupBy(\"aircraft\")  # Group by manufacturer\n",
    "    .agg(count(\"*\").alias(\"active_flight_count\"))  # Count active flights\n",
    "    .withColumn(  # Add a rank column to identify the top manufacturer\n",
    "        \"rank\",\n",
    "        row_number().over(Window.orderBy(col(\"active_flight_count\").desc()))\n",
    "    )\n",
    "    .filter(col(\"rank\") == 1)  # Filter to keep only the top manufacturer\n",
    "    .drop(\"rank\")  # Drop the rank column\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select necessary columns\n",
    "top_manufacturer = top_manufacturer.select(\"aircraft\", \"active_flight_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|aircraft|active_flight_count|\n",
      "+--------+-------------------+\n",
      "|    B77W|                188|\n",
      "+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_manufacturer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 03:22:26,692 - INFO - Loading data for flights from ./data/ingested_data\\flights\\tech_year=2024\\tech_month=12\\tech_day=15\\tech_hour=03\n",
      "2024-12-15 03:22:26,774 - INFO - Loading data for airlines from ./data/ingested_data\\airlines\\tech_year=2024\\tech_month=12\\tech_day=15\\tech_hour=03\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df_flights = load_latest_data(spark, base_extracted_path, \"flights\")\n",
    "df_airlines = load_latest_data(spark, base_extracted_path, \"airlines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----------+------------------+----------------------+------------+\n",
      "|flightID|aircraft|airlineCode|airportOrigineCode|airportDestinationCode|flightStatus|\n",
      "+--------+--------+-----------+------------------+----------------------+------------+\n",
      "|3857547f|    B77W|        QTR|               DOH|                   HKG|           0|\n",
      "|3857e59a|    B763|        FDX|               RNO|                   OAK|           1|\n",
      "|3857e905|    B77L|        FDX|               LGG|                   OAK|           1|\n",
      "|38581e7d|    A359|        ETH|               PVG|                   ADD|           0|\n",
      "|38582e0d|    MD11|        FDX|               OAK|                   ANC|           1|\n",
      "|38583038|    B763|        FDX|               MEM|                   OAK|           1|\n",
      "|38583115|    A359|        DLH|               PVG|                   MUC|           0|\n",
      "|38583161|    B789|        MSR|               PVG|                   CAI|           0|\n",
      "|38583688|    A359|        SIA|               SIN|                   JFK|           0|\n",
      "|3858372c|    B77W|        THY|               ICN|                   IST|           0|\n",
      "|3858376c|    A359|        SJX|               TPE|                   SFO|           0|\n",
      "|38583d05|    A359|        CCA|               PEK|                   MEL|           0|\n",
      "|3858403d|    B789|        CSH|               PVG|                   BUD|           0|\n",
      "|3858424a|    B77W|        THY|               TPE|                   IST|           0|\n",
      "|385842a7|    AT75|        NVQ|               DAC|                   CXB|           0|\n",
      "|385860b2|    B748|        CCA|               PEK|                   JFK|           0|\n",
      "|38586865|    B789|        AFR|               PEK|                   CDG|           0|\n",
      "|38587304|    B789|        QFA|               PER|                   LHR|           0|\n",
      "|385873ba|    B789|        DKH|               PVG|                   BRU|           0|\n",
      "|38587541|    B77W|        AFR|               PVG|                   CDG|           0|\n",
      "+--------+--------+-----------+------------------+----------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_flights.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Country` cannot be resolved. Did you mean one of the following? [`Code`, `ICAO`, `Name`].;\n'Project [ICAO#444 AS airlineCode#449, 'Country AS airlineCountry#450]\n+- Relation [Code#443,ICAO#444,Name#445] parquet\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m df_flights \u001b[38;5;241m=\u001b[39m df_flights\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mdf_airlines\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mICAO\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mairlineCode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCountry\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mairlineCountry\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m      6\u001b[0m     on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mairlineCode\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m     how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\tefte\\anaconda3\\envs\\kata-env\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:3229\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   3184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   3185\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   3186\u001b[0m \n\u001b[0;32m   3187\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3227\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[0;32m   3228\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3229\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32mc:\\Users\\tefte\\anaconda3\\envs\\kata-env\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\tefte\\anaconda3\\envs\\kata-env\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Country` cannot be resolved. Did you mean one of the following? [`Code`, `ICAO`, `Name`].;\n'Project [ICAO#444 AS airlineCode#449, 'Country AS airlineCountry#450]\n+- Relation [Code#443,ICAO#444,Name#445] parquet\n"
     ]
    }
   ],
   "source": [
    "df_flights = df_flights.join(\n",
    "    df_airlines.select(\n",
    "        col(\"ICAO\").alias(\"airlineCode\"),\n",
    "        col(\"Country\").alias(\"airlineCountry\")\n",
    "    ),\n",
    "    on=\"airlineCode\",\n",
    "    how=\"left\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kata-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
